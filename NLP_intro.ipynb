{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc75a6ef-21f4-484a-bd5d-be361d928066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Juan\\\\nlp_intro'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced12bb7-87f3-48ff-a737-1e3fa51d5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:\\\\Jupyter projects\\\\nlp_intro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c7eb14-90a5-48ac-94ac-4b97d40bbaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Jupyter projects\\\\nlp_intro'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fbacb4-ab86-45a2-9f95-aa6faf3cebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3060 (UUID: GPU-e8861a5c-89dd-5ee2-06b2-d3bca4ee7bc6)\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8441e5a7-141e-45b0-b351-1133b3b91e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-10-31 12:20:40--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10246 (10K) [text/plain]\n",
      "Saving to: 'helper_functions.py.2'\n",
      "\n",
      "     0K ..........                                            100% 2.95M=0.003s\n",
      "\n",
      "2022-10-31 12:20:40 (2.95 MB/s) - 'helper_functions.py.2' saved [10246/10246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download helper functions script\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a604ebd5-a459-4e59-82f2-8aa3b51f6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import series of helper functions for the notebook\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721900ea-388d-449c-b28c-837d42f365de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-10-31 12:20:45--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.212.240, 142.250.179.240, 142.250.180.16, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.212.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607343 (593K) [application/zip]\n",
      "Saving to: 'nlp_getting_started.zip.3'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  8% 2.07M 0s\n",
      "    50K .......... .......... .......... .......... .......... 16% 3.60M 0s\n",
      "   100K .......... .......... .......... .......... .......... 25% 7.24M 0s\n",
      "   150K .......... .......... .......... .......... .......... 33% 9.22M 0s\n",
      "   200K .......... .......... .......... .......... .......... 42% 16.4M 0s\n",
      "   250K .......... .......... .......... .......... .......... 50% 1.96M 0s\n",
      "   300K .......... .......... .......... .......... .......... 59% 56.0M 0s\n",
      "   350K .......... .......... .......... .......... .......... 67% 71.6M 0s\n",
      "   400K .......... .......... .......... .......... .......... 75% 72.7M 0s\n",
      "   450K .......... .......... .......... .......... .......... 84% 65.9M 0s\n",
      "   500K .......... .......... .......... .......... .......... 92% 64.7M 0s\n",
      "   550K .......... .......... .......... .......... ...       100%  101M=0.08s\n",
      "\n",
      "2022-10-31 12:20:45 (7.14 MB/s) - 'nlp_getting_started.zip.3' saved [607343/607343]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data (same as from Kaggle)\n",
    "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
    "\n",
    "# Unzip data\n",
    "unzip_data(\"nlp_getting_started.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37069212-a601-4fad-b3c6-200ff09518cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn .csv files into pandas DataFrame's\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4de20eab-5987-4475-8727-4cf3b4d74a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41db42b8-cb17-485a-80d8-d98dcab61ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The test data doesn't have a target (that's what we'd try to predict)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00ca1756-1f4e-406c-b338-8a605ea5abb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class?\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ad19921-f72d-4a2e-adf8-dcb2cc175198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 7613\n",
      "Total test samples: 3263\n",
      "Total samples: 10876\n"
     ]
    }
   ],
   "source": [
    "# How many samples total?\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33d97aaa-4cd0-4a11-ae90-4d028c91af90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Newberg upheaval: Jacque Betz responds 'looking forward to the day' she can answerÛ_ http://t.co/LzasR05ljo  #news http://t.co/IeMxGSE2BE\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "cancel the fucking show. Evacuate MetLife  https://t.co/SkQ8oUcM3R\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Sooo police dispatch said there was a person threatening to shoot up the Walmart on Rutherford &amp; they had to evacuate\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "L B #Entertainment lot of 8 #BruceWillis MOVIES #DVD DIE HARD 1 2 12 MONKEYS ARMAGEDDON SIXTH #eBay #Auction http://t.co/CxDJApzXMP\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "A sandstorm in Jordan has coated the Middle EastÛªs largest refugee camp in a layer of grit http://t.co/hVJmuuaLXV http://t.co/T8Nz6h9Zz4\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ca785d-d9cb-409b-ac78-92285061b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
    "                                                                            random_state=42) # random state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc9ecb24-abc3-4174-9717-f5533fef5b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first 10 training sentences and their labels\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b49ca741-a54a-42af-b1f8-b5ce5fdf2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94b149dc-539b-4bec-8385-08d9d98f5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = TextVectorization(max_tokens=None,\n",
    "                                    standardize=\"lower_and_strip_punctuation\",\n",
    "                                    split=\"whitespace\",\n",
    "                                    ngrams=None,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ed86d0-b66d-40cd-a660-f172a413cf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find average number of tokens/words in training tweets\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e24040e-0a8c-4da4-a5b9-489b69d5532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode='int',\n",
    "                                    output_sequence_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bd30c6a-e0a5-4053-b654-0d30917edf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbb8b9fc-0981-4932-84cf-41c21b95d303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create sample sentence\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1e05fae-11ac-459b-b1cc-91ef583152cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "'Redskins WR Roberts Belly-Bombed ' via @TeamStream http://t.co/GbcvVEvDTY      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[9261, 6668, 3461,    1,   49, 4457,    1,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choose random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed4630ca-8941-4935-8d1b-4a4164a510ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "#Get unique words in vocab\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5] #5 most common tokens\n",
    "bottom_5_words = words_in_vocab[-5:]# 5 least common tokens\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07cf86f0-79d9-408f-80be-fbd5fc5880d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x20337d59460>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                             output_dim=128,\n",
    "                             embeddings_initializer='uniform',\n",
    "                             input_length=max_length,\n",
    "                             name='embedding_1'\n",
    "                            )\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cf6efa1-3e23-4444-8a94-c1476c728ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "@crabbycale OH MY GOD THE MEMORIES ARE FLOODING BACK      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.04419341, -0.02349438,  0.00897091, ..., -0.02296398,\n",
       "          0.03352927, -0.03165143],\n",
       "        [-0.00158058,  0.04919602, -0.03033113, ..., -0.02477709,\n",
       "          0.03940349,  0.03351363],\n",
       "        [ 0.01360643,  0.01858305,  0.01822582, ..., -0.02490723,\n",
       "         -0.0353555 , -0.0194996 ],\n",
       "        ...,\n",
       "        [-0.03226759, -0.00171774,  0.0351618 , ...,  0.02675134,\n",
       "          0.01835357,  0.03775084],\n",
       "        [-0.03226759, -0.00171774,  0.0351618 , ...,  0.02675134,\n",
       "          0.01835357,  0.03775084],\n",
       "        [-0.03226759, -0.00171774,  0.0351618 , ...,  0.02675134,\n",
       "          0.01835357,  0.03775084]]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "#embed random sentence\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f60aabbd-abaa-4bb2-8a0f-a19f08fcdf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([ 0.04419341, -0.02349438,  0.00897091, -0.04458587, -0.01814613,\n",
       "        0.00300959, -0.02228291, -0.00437361,  0.02259595, -0.00011273,\n",
       "       -0.04051753,  0.00835348, -0.04182898,  0.0056477 ,  0.01021483,\n",
       "       -0.00122981, -0.02769551,  0.03072769, -0.00697983,  0.0458906 ,\n",
       "        0.01045223,  0.00588731,  0.00919248,  0.00808529, -0.0021399 ,\n",
       "        0.02289527, -0.01313497, -0.01990033, -0.03011679, -0.00606819,\n",
       "        0.04393696, -0.03031437, -0.02962606,  0.01701239, -0.01730274,\n",
       "        0.04995744, -0.04426212, -0.03533043, -0.00429139,  0.02145872,\n",
       "       -0.04886849, -0.03404929, -0.03237206,  0.02334782,  0.01943706,\n",
       "        0.03244599, -0.00215797,  0.03356018, -0.02156054, -0.02636491,\n",
       "        0.00458449, -0.01652144,  0.02260948, -0.01586981, -0.03174639,\n",
       "       -0.03823803, -0.03235634, -0.00475132,  0.04063277, -0.04728155,\n",
       "       -0.00602496,  0.02929956,  0.02735356, -0.04892446, -0.04926798,\n",
       "       -0.04977253, -0.02738794,  0.00483299, -0.03521509,  0.00117003,\n",
       "        0.02410886, -0.03416122, -0.03871933,  0.02031428,  0.0062964 ,\n",
       "       -0.02964767,  0.04220674, -0.01546793, -0.01053908, -0.01531087,\n",
       "       -0.00540706,  0.03334621,  0.01045731,  0.00207491, -0.03129255,\n",
       "       -0.02048042,  0.03848629,  0.02842008,  0.04459219, -0.04447516,\n",
       "        0.01749624, -0.03388571, -0.00341982, -0.02531081,  0.03499847,\n",
       "       -0.00967081,  0.00583767,  0.00807232,  0.01967451, -0.03010404,\n",
       "        0.03629836,  0.01954521,  0.00418577, -0.04349959, -0.04431374,\n",
       "        0.01673431, -0.00616195, -0.00883669,  0.04140501,  0.04699573,\n",
       "       -0.03184012,  0.043664  , -0.04947168,  0.0326043 , -0.02029322,\n",
       "       -0.03853459,  0.0367    ,  0.03231946, -0.01920339,  0.02601602,\n",
       "       -0.00385372,  0.04840043, -0.00418269,  0.01799739, -0.00443996,\n",
       "       -0.02296398,  0.03352927, -0.03165143], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embed[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35ff83bc-7e2d-4ef8-be41-c427ae0b7e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#create tokenization and modelling pipeline\n",
    "\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06d78448-c938-4d40-814d-27b241bc0cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8855eb11-dd1c-417c-9f22-ba0aaeb48560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8b7a292-8fd3-4442-a6eb-f1c6a021de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "  \n",
    "    Args:\n",
    "    -----\n",
    "    y_true = true labels in the form of a 1D array\n",
    "    y_pred = predicted labels in the form of a 1D array\n",
    "  \n",
    "    Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "    \"\"\"\n",
    "    # Calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    \n",
    "    # Calculate model precision, recall and f1 score using \"weighted\" average\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                    \"precision\": model_precision,\n",
    "                    \"recall\": model_recall,\n",
    "                    \"f1\": model_f1}\n",
    "    \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9582e90d-2763-435b-b1d6-cc23db23f486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels,\n",
    "                                     y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b32762ce-87aa-4ce4-a21c-02390be1d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 1\n",
    "\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f510e280-9e54-4242-b31c-ec1cde6e99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype='string') # inputs are 1D strings\n",
    "x = text_vectorizer(inputs) # text input into numbers\n",
    "x = embedding(x) # create embedding of numerized numbers\n",
    "x = layers.GlobalAveragePooling1D()(x) # \n",
    "\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_1 = tf.keras.Model(inputs, outputs, name='model_1_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb4a3cde-612b-4e28-b2b4-45b9c682abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af581f62-dce9-45a6-9d3a-8b6c0e802ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2dc9a4c-b9a7-47ae-8dd5-e07fa70bbc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/simple_dense_model/20221031-143555\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 7ms/step - loss: 0.6098 - accuracy: 0.6945 - val_loss: 0.5365 - val_accuracy: 0.7520\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4415 - accuracy: 0.8187 - val_loss: 0.4695 - val_accuracy: 0.7874\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3467 - accuracy: 0.8603 - val_loss: 0.4593 - val_accuracy: 0.7913\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2852 - accuracy: 0.8918 - val_loss: 0.4644 - val_accuracy: 0.7887\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2383 - accuracy: 0.9124 - val_loss: 0.4769 - val_accuracy: 0.7874\n"
     ]
    }
   ],
   "source": [
    "model_1_history = model_1.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                                    experiment_name=\"simple_dense_model\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f00e4b0b-b5d0-40fe-b374-6fbc48f586dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4769 - accuracy: 0.7874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47693932056427, 0.787401556968689]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the results\n",
    "model_1.evaluate(val_sentences, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e513f3d2-fc38-4854-90cd-081b0d0e03f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_1/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n",
       " array([[-0.04223466, -0.01232036,  0.02492392, ...,  0.03739414,\n",
       "          0.02754631,  0.0269674 ],\n",
       "        [ 0.03788092, -0.02968534,  0.00245902, ..., -0.01668192,\n",
       "          0.03793933, -0.03829484],\n",
       "        [-0.04201521, -0.03491365, -0.00389276, ...,  0.06751584,\n",
       "          0.07394353, -0.04339752],\n",
       "        ...,\n",
       "        [ 0.01947466,  0.01368252, -0.01293451, ...,  0.04440505,\n",
       "          0.04755535,  0.04615953],\n",
       "        [-0.06935913, -0.0707734 , -0.07123754, ...,  0.03798301,\n",
       "          0.05890796, -0.00278805],\n",
       "        [-0.11110515, -0.02027354, -0.11476819, ...,  0.08226258,\n",
       "          0.0864292 , -0.04234757]], dtype=float32)>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d49cc94-72aa-40c1-8388-59f1dd370423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "print(embed_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6e0505a-1029-42d7-94d2-301e6ea6d65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4020229 ],\n",
       "       [0.7458195 ],\n",
       "       [0.99794346],\n",
       "       [0.10876183],\n",
       "       [0.11108602],\n",
       "       [0.94066375],\n",
       "       [0.9065157 ],\n",
       "       [0.9925392 ],\n",
       "       [0.96853405],\n",
       "       [0.26828274]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions (these come back in the form of probabilities)\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs[:10] # only print out the first 10 prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ce85c3f-5c2c-44c3-9ed6-826d8420e50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn prediction probabilities into single-dimension tensor of floats\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\n",
    "model_1_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dff6f2f-c520-4f8b-b5fd-00b981731811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.74015748031496,\n",
       " 'precision': 0.7914920592553047,\n",
       " 'recall': 0.7874015748031497,\n",
       " 'f1': 0.7846966492209201}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_1 metrics\n",
    "model_1_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c47e7016-6405-44a7-8cdd-8edb45f3f5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is our simple Keras model better than our baseline model?\n",
    "import numpy as np\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "336accc2-83ab-44bb-927c-76a097ccd3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 78.74, Difference: -0.52\n",
      "Baseline precision: 0.81, New precision: 0.79, Difference: -0.02\n",
      "Baseline recall: 0.79, New recall: 0.79, Difference: -0.01\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.00\n"
     ]
    }
   ],
   "source": [
    "# Create a helper function to compare our baseline results to new model results\n",
    "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
    "    for key, value in baseline_results.items():\n",
    "        print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")\n",
    "\n",
    "compare_baseline_to_new_results(baseline_results=baseline_results, \n",
    "                                new_model_results=model_1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "986dd6a1-9aae-4407-a9e3-38639fa14325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "723f291c-b4c5-4ed2-a318-07aed355c5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "420e577e-3bda-4f4d-833e-d7e0b8cf1674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Get the weight matrix of embedding layer \n",
    "# (these are the numerical patterns between the text in the training dataset the model has learned)\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "print(embed_weights.shape) # same size as vocab size and embedding_dim (each word is a embedding_dim size vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b391129-7b32-4cab-a7e6-5954e879f2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15, 128)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer='uniform',\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_2\"\n",
    "                                    )\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_2_embedding(x)\n",
    "print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "print(x.shape)\n",
    "outputs=layers.Dense(1, activation = 'sigmoid')(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6c9b757-ca0c-4aa9-8aec-eb010014266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b088b4b2-97dd-40a8-addc-a77ec9173d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a367b5ce-fd56-44e5-9e9f-6a3a382af71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/LSTM/20221031-172408\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 10ms/step - loss: 0.5136 - accuracy: 0.7393 - val_loss: 0.4599 - val_accuracy: 0.7782\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.3205 - accuracy: 0.8704 - val_loss: 0.5084 - val_accuracy: 0.7808\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.2211 - accuracy: 0.9156 - val_loss: 0.5811 - val_accuracy: 0.7612\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.1572 - accuracy: 0.9444 - val_loss: 0.6095 - val_accuracy: 0.7730\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.1117 - accuracy: 0.9583 - val_loss: 0.7912 - val_accuracy: 0.7572\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_2_history = model_2.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
    "                                                                     \"LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8d81c42-5e79-47dd-8268-8892be869bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((762, 1),\n",
       " array([[0.02688102],\n",
       "        [0.71889037],\n",
       "        [0.9995449 ],\n",
       "        [0.0646049 ],\n",
       "        [0.00470248],\n",
       "        [0.999308  ],\n",
       "        [0.85061496],\n",
       "        [0.9997037 ],\n",
       "        [0.9995945 ],\n",
       "        [0.20401447]], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9be5faf-28dc-440d-9084-655b4c838c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round out predictions and reduce to 1-dimensional array\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14f68349-af93-4d89-85a8-bd1d271df199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.7217847769029,\n",
       " 'precision': 0.7579616566971975,\n",
       " 'recall': 0.7572178477690289,\n",
       " 'f1': 0.7553466343855008}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Calculate LSTM model results\n",
    "model_2_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6bc31e7-e627-46cc-bfe1-2bb3c18f0161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 75.72, Difference: -3.54\n",
      "Baseline precision: 0.81, New precision: 0.76, Difference: -0.05\n",
      "Baseline recall: 0.79, New recall: 0.76, Difference: -0.04\n",
      "Baseline f1: 0.79, New f1: 0.76, Difference: -0.03\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results, model_2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77797f51-22b0-455e-ba14-2463ebd05729",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer='uniform',\n",
    "                                     input_length=max_length,\n",
    "                                     name='embedding_3'\n",
    "                                     )\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_3_embedding(x)\n",
    "x = layers.GRU(64)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cebacf93-5f7c-4878-9ee7-882529a06145",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(loss=\"binary_crossentropy\",\n",
    "                          optimizer=tf.keras.optimizers.Adam(),\n",
    "                          metrics=['accuracy']\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9d0a764-f3af-427b-8214-42b42e96c952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d4ffdfd-2cf3-4a79-917b-3031202faf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/GRU/20221031-175908\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 3s 9ms/step - loss: 0.5299 - accuracy: 0.7221 - val_loss: 0.4535 - val_accuracy: 0.7769\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.3198 - accuracy: 0.8680 - val_loss: 0.4888 - val_accuracy: 0.7795\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.2189 - accuracy: 0.9177 - val_loss: 0.5626 - val_accuracy: 0.7795\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.1562 - accuracy: 0.9463 - val_loss: 0.6227 - val_accuracy: 0.7769\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.1194 - accuracy: 0.9600 - val_loss: 0.5949 - val_accuracy: 0.7717\n"
     ]
    }
   ],
   "source": [
    "model_3_history = model_3.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5, \n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"GRU\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0d765499-a723-4925-a17d-6359625eb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((762, 1),\n",
       " array([[0.3066615 ],\n",
       "        [0.90319353],\n",
       "        [0.9966588 ],\n",
       "        [0.14305897],\n",
       "        [0.01514528],\n",
       "        [0.9873284 ],\n",
       "        [0.6025643 ],\n",
       "        [0.9967751 ],\n",
       "        [0.9953602 ],\n",
       "        [0.37042946]], dtype=float32))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation data\n",
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs.shape, model_3_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49c9f2ba-fb70-4293-a241-8d82b17ae233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to prediction classes\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9bd8932a-70a9-4233-8921-f3a9b1b5d32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.16535433070865,\n",
       " 'precision': 0.7715668446582516,\n",
       " 'recall': 0.7716535433070866,\n",
       " 'f1': 0.7706989403422689}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcuate model_3 results\n",
    "model_3_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33c8b48d-814d-420f-9958-faf81d86df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 77.17, Difference: -2.10\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.04\n",
      "Baseline recall: 0.79, New recall: 0.77, Difference: -0.02\n",
      "Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"
     ]
    }
   ],
   "source": [
    "# Compare to baseline\n",
    "compare_baseline_to_new_results(baseline_results, model_3_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0e3d92f-7910-42d7-8afd-93f1aa918d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_4_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                    output_dim=128,\n",
    "                                    embeddings_initializer='uniform',\n",
    "                                    input_length=max_length,\n",
    "                                    name='embedding_4')\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_4_embedding(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name='model_4_Bidirectional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2707f48f-8f50-4c69-a841-6d90e414e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(loss='binary_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy']\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5ec5b93-1702-4808-b53a-d712db3d34b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3830c75d-83e6-45c8-ab46-8ee9c6326b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/bidirectional_RNN/20221031-181524\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 13ms/step - loss: 0.5128 - accuracy: 0.7414 - val_loss: 0.4594 - val_accuracy: 0.7808\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.3154 - accuracy: 0.8718 - val_loss: 0.5155 - val_accuracy: 0.7756\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.2136 - accuracy: 0.9194 - val_loss: 0.5762 - val_accuracy: 0.7690\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.1454 - accuracy: 0.9518 - val_loss: 0.6493 - val_accuracy: 0.7756\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.1027 - accuracy: 0.9647 - val_loss: 0.6973 - val_accuracy: 0.7664\n"
     ]
    }
   ],
   "source": [
    "model_4_history = model_4.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"bidirectional_RNN\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b04c8330-7cdf-457e-9c6f-a0df9df46181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.04355242],\n",
       "       [0.8433966 ],\n",
       "       [0.9995875 ],\n",
       "       [0.10725769],\n",
       "       [0.00377868],\n",
       "       [0.99506384],\n",
       "       [0.97462577],\n",
       "       [0.99960226],\n",
       "       [0.99953735],\n",
       "       [0.19197886]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07ce30a8-1baa-48b0-854d-3c7e18bb15a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3cb1a26b-47c5-4d40-b7b2-b7aac88ecdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.64041994750657,\n",
       " 'precision': 0.7664644290702826,\n",
       " 'recall': 0.7664041994750657,\n",
       " 'f1': 0.765226875463096}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_results = calculate_results(val_labels, model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8cc5ac9c-622e-470f-ab55-67148dabc736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 76.64, Difference: -2.62\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.04\n",
      "Baseline recall: 0.79, New recall: 0.77, Difference: -0.03\n",
      "Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results, model_4_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84e915b4-7297-407c-a903-2b7baea3eef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"]))\n",
    "conv_1d = layers.Conv1D(filters=32, kernel_size=5, activation='relu')\n",
    "conv_1d_output = conv_1d(embedding_test)\n",
    "max_pool = layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output)\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f70e59fe-5acd-4efa-b3cc-e3fbcd2deb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       " array([[[ 0.03528265, -0.02572645, -0.00296372, ..., -0.00724024,\n",
       "          -0.00215557,  0.01195835],\n",
       "         [-0.07590447,  0.00535872,  0.00499065, ..., -0.01406056,\n",
       "           0.04207451, -0.05433482],\n",
       "         [-0.04285926, -0.00275673,  0.03422695, ..., -0.03037849,\n",
       "           0.03506533, -0.01866705],\n",
       "         ...,\n",
       "         [-0.04223466, -0.01232036,  0.02492392, ...,  0.03739414,\n",
       "           0.02754631,  0.0269674 ],\n",
       "         [-0.04223466, -0.01232036,  0.02492392, ...,  0.03739414,\n",
       "           0.02754631,  0.0269674 ],\n",
       "         [-0.04223466, -0.01232036,  0.02492392, ...,  0.03739414,\n",
       "           0.02754631,  0.0269674 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n",
       " array([[[0.        , 0.01005853, 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.02544659, 0.        , 0.08993326, 0.01290459,\n",
       "          0.049881  , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.03128093,\n",
       "          0.        , 0.        , 0.05201892, 0.        , 0.03996772,\n",
       "          0.06395552, 0.06511808, 0.07293111, 0.        , 0.        ,\n",
       "          0.08802439, 0.04550944],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.12016767, 0.03642854, 0.        , 0.        , 0.0191033 ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.04579643, 0.07496621, 0.        , 0.        , 0.09703445,\n",
       "          0.06451636, 0.04703507, 0.05728553, 0.        , 0.02675958,\n",
       "          0.        , 0.03111557, 0.01356664, 0.        , 0.        ,\n",
       "          0.01060911, 0.        ],\n",
       "         [0.        , 0.03527611, 0.        , 0.        , 0.04265239,\n",
       "          0.        , 0.12292225, 0.        , 0.06469676, 0.02486412,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.02871974, 0.0370989 , 0.        , 0.01708161, 0.0948524 ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.05250585,\n",
       "          0.        , 0.0641699 , 0.        , 0.        , 0.01733043,\n",
       "          0.08849619, 0.01657816],\n",
       "         [0.        , 0.15223236, 0.00637049, 0.        , 0.        ,\n",
       "          0.01528506, 0.0006901 , 0.        , 0.        , 0.00777935,\n",
       "          0.        , 0.        , 0.010458  , 0.        , 0.        ,\n",
       "          0.13492757, 0.04690388, 0.        , 0.03101754, 0.12024355,\n",
       "          0.09096271, 0.03617558, 0.        , 0.        , 0.04747348,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.02712559, 0.        , 0.        , 0.        ,\n",
       "          0.04649017, 0.09548926, 0.        , 0.01787855, 0.01119552,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.01218968, 0.03190016, 0.01802211, 0.        , 0.11088604,\n",
       "          0.06165909, 0.03577453, 0.        , 0.        , 0.01757489,\n",
       "          0.        , 0.00716926, 0.        , 0.        , 0.        ,\n",
       "          0.01367918, 0.        ],\n",
       "         [0.        , 0.04775164, 0.0187391 , 0.        , 0.        ,\n",
       "          0.03662689, 0.06160329, 0.        , 0.0108437 , 0.00718853,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.02360181, 0.05217686, 0.        , 0.        , 0.08045706,\n",
       "          0.02009838, 0.        , 0.        , 0.        , 0.01969029,\n",
       "          0.00708447, 0.01013829, 0.        , 0.        , 0.        ,\n",
       "          0.01940051, 0.        ],\n",
       "         [0.        , 0.04775164, 0.0187391 , 0.        , 0.        ,\n",
       "          0.03662689, 0.06160329, 0.        , 0.0108437 , 0.00718853,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.02360181, 0.05217686, 0.        , 0.        , 0.08045706,\n",
       "          0.02009838, 0.        , 0.        , 0.        , 0.01969029,\n",
       "          0.00708447, 0.01013829, 0.        , 0.        , 0.        ,\n",
       "          0.01940051, 0.        ],\n",
       "         [0.        , 0.04775164, 0.0187391 , 0.        , 0.        ,\n",
       "          0.03662689, 0.06160329, 0.        , 0.0108437 , 0.00718853,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.02360181, 0.05217686, 0.        , 0.        , 0.08045706,\n",
       "          0.02009838, 0.        , 0.        , 0.        , 0.01969029,\n",
       "          0.00708447, 0.01013829, 0.        , 0.        , 0.        ,\n",
       "          0.01940051, 0.        ],\n",
       "         [0.        , 0.04775164, 0.0187391 , 0.        , 0.        ,\n",
       "          0.03662689, 0.06160329, 0.        , 0.0108437 , 0.00718853,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.02360181, 0.05217686, 0.        , 0.        , 0.08045706,\n",
       "          0.02009838, 0.        , 0.        , 0.        , 0.01969029,\n",
       "          0.00708447, 0.01013829, 0.        , 0.        , 0.        ,\n",
       "          0.01940051, 0.        ],\n",
       "         [0.        , 0.04775164, 0.0187391 , 0.        , 0.        ,\n",
       "          0.03662689, 0.06160329, 0.        , 0.0108437 , 0.00718853,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.02360181, 0.05217686, 0.        , 0.        , 0.08045706,\n",
       "          0.02009838, 0.        , 0.        , 0.        , 0.01969029,\n",
       "          0.00708447, 0.01013829, 0.        , 0.        , 0.        ,\n",
       "          0.01940051, 0.        ],\n",
       "         [0.        , 0.04775164, 0.0187391 , 0.        , 0.        ,\n",
       "          0.03662689, 0.06160329, 0.        , 0.0108437 , 0.00718853,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.02360181, 0.05217686, 0.        , 0.        , 0.08045706,\n",
       "          0.02009838, 0.        , 0.        , 0.        , 0.01969029,\n",
       "          0.00708447, 0.01013829, 0.        , 0.        , 0.        ,\n",
       "          0.01940051, 0.        ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       " array([[0.        , 0.15223236, 0.0187391 , 0.        , 0.04265239,\n",
       "         0.12016767, 0.12292225, 0.        , 0.08993326, 0.02486412,\n",
       "         0.049881  , 0.        , 0.010458  , 0.        , 0.        ,\n",
       "         0.13492757, 0.07496621, 0.01802211, 0.03101754, 0.12024355,\n",
       "         0.09096271, 0.04703507, 0.05728553, 0.        , 0.05250585,\n",
       "         0.06395552, 0.06511808, 0.07293111, 0.        , 0.01733043,\n",
       "         0.08849619, 0.04550944]], dtype=float32)>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the outputs of each layer\n",
    "embedding_test[:1], conv_1d_output[:1], max_pool_output[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "88d14a18-88b6-467e-a178-11489a0407c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 11, 32)            20512     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,300,545\n",
      "Trainable params: 1,300,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_5_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer='uniform',\n",
    "                                     input_length=max_length,\n",
    "                                     name='embedding_5')\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_5_embedding(x)\n",
    "x = layers.Conv1D(filters=32, kernel_size=5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name='model_5_Conv1D')\n",
    "\n",
    "model_5.compile(loss='binary_crossentropy',\n",
    "                optimizer='Adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_5.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e09e1d7-ea38-466f-81d3-7293e9e22e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/Conv1D/20221031-194318\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 7ms/step - loss: 0.5729 - accuracy: 0.7069 - val_loss: 0.4738 - val_accuracy: 0.7822\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3439 - accuracy: 0.8580 - val_loss: 0.4682 - val_accuracy: 0.7979\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2107 - accuracy: 0.9219 - val_loss: 0.5301 - val_accuracy: 0.7730\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.1344 - accuracy: 0.9553 - val_loss: 0.6019 - val_accuracy: 0.7703\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0956 - accuracy: 0.9682 - val_loss: 0.6570 - val_accuracy: 0.7913\n"
     ]
    }
   ],
   "source": [
    "model_5_history = model_5.fit(train_sentences,\n",
    "                             train_labels,\n",
    "                             epochs=5,\n",
    "                             validation_data=(val_sentences, val_labels),\n",
    "                             callbacks=[create_tensorboard_callback(SAVE_DIR, 'Conv1D')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a063330-8ad2-4780-8c14-0570f17d7c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4409066 ],\n",
       "       [0.645164  ],\n",
       "       [0.9998053 ],\n",
       "       [0.08580104],\n",
       "       [0.001558  ]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b208f0a1-eb46-437c-94c6-fae1b4e3e8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 1., 1., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1b4cd4b8-d70d-48a8-8775-991825ba618d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.13385826771653,\n",
       " 'precision': 0.7939542015160368,\n",
       " 'recall': 0.7913385826771654,\n",
       " 'f1': 0.7892663801484977}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_5_preds)\n",
    "\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "083ed313-5b4a-4600-9a3d-21aef9c724bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 79.13, Difference: -0.13\n",
      "Baseline precision: 0.81, New precision: 0.79, Difference: -0.02\n",
      "Baseline recall: 0.79, New recall: 0.79, Difference: -0.00\n",
      "Baseline f1: 0.79, New f1: 0.79, Difference: 0.00\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results, model_5_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc262373-2719-4986-b4bc-d775ec5c062c",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nJIT compilation failed.\n\t [[{{node EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/embedding_lookup/mod}}]] [Op:__inference_restored_function_body_57069]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20076/3667629617.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://tfhub.dev/google/universal-sentence-encoder/4\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# load Universal Sentence Encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m embed_samples = embed([sample_sentence,\n\u001b[0m\u001b[0;32m      4\u001b[0m                       \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[1;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Graph execution error:\n\nJIT compilation failed.\n\t [[{{node EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/embedding_lookup/mod}}]] [Op:__inference_restored_function_body_57069]"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder\n",
    "embed_samples = embed([sample_sentence,\n",
    "                      \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
    "\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3aaad7-67e1-4078-ba40-6feacf773cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each sentence has been encoded into a 512 dimension vector\n",
    "embed_samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "03471131-52a6-4779-831e-b6bd2fc09b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[],\n",
    "                                        dtype=tf.string,\n",
    "                                        trainable=False,\n",
    "                                        name='USE'\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1f297fed-f677-45de-9665-4f6aeb1dd320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "], name='model_6_USE')\n",
    "\n",
    "model_6.compile(loss='binary_crossentropy',\n",
    "                optimizer='Adam',\n",
    "                metrics=['accuracy']\n",
    "               )\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d2d2c799-034f-4fd9-90ce-5592e9fa6612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/tf_hub_sentence_encoder/20221031-201237\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nJIT compilation failed.\n\t [[{{node EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/embedding_lookup/mod}}]] [Op:__inference_train_function_67092]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20076/830842737.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model_6_history = model_6.fit(train_sentences,\n\u001b[0m\u001b[0;32m      2\u001b[0m                               \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                               callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
      "\u001b[1;32mD:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Graph execution error:\n\nJIT compilation failed.\n\t [[{{node EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/embedding_lookup/mod}}]] [Op:__inference_train_function_67092]"
     ]
    }
   ],
   "source": [
    "model_6_history = model_6.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
    "                                                                     \"tf_hub_sentence_encoder\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bacc23-1316-48a8-95cd-91c4d42b444b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
