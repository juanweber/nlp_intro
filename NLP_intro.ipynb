{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc75a6ef-21f4-484a-bd5d-be361d928066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Juan\\\\nlp_intro'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced12bb7-87f3-48ff-a737-1e3fa51d5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:\\\\Jupyter projects\\\\nlp_intro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c7eb14-90a5-48ac-94ac-4b97d40bbaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Jupyter projects\\\\nlp_intro'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fbacb4-ab86-45a2-9f95-aa6faf3cebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3060 (UUID: GPU-e8861a5c-89dd-5ee2-06b2-d3bca4ee7bc6)\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8441e5a7-141e-45b0-b351-1133b3b91e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-10-31 20:20:54--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10246 (10K) [text/plain]\n",
      "Saving to: 'helper_functions.py.3'\n",
      "\n",
      "     0K ..........                                            100% 3.33M=0.003s\n",
      "\n",
      "2022-10-31 20:20:54 (3.33 MB/s) - 'helper_functions.py.3' saved [10246/10246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download helper functions script\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a604ebd5-a459-4e59-82f2-8aa3b51f6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import series of helper functions for the notebook\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721900ea-388d-449c-b28c-837d42f365de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-10-31 20:21:03--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 66.102.1.128, 142.251.5.128, 64.233.167.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|66.102.1.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607343 (593K) [application/zip]\n",
      "Saving to: 'nlp_getting_started.zip.4'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  8% 1.31M 0s\n",
      "    50K .......... .......... .......... .......... .......... 16% 2.61M 0s\n",
      "   100K .......... .......... .......... .......... .......... 25% 5.51M 0s\n",
      "   150K .......... .......... .......... .......... .......... 33% 8.73M 0s\n",
      "   200K .......... .......... .......... .......... .......... 42% 7.26M 0s\n",
      "   250K .......... .......... .......... .......... .......... 50% 10.4M 0s\n",
      "   300K .......... .......... .......... .......... .......... 59% 15.0M 0s\n",
      "   350K .......... .......... .......... .......... .......... 67% 12.1M 0s\n",
      "   400K .......... .......... .......... .......... .......... 75% 24.1M 0s\n",
      "   450K .......... .......... .......... .......... .......... 84% 4.48M 0s\n",
      "   500K .......... .......... .......... .......... .......... 92% 8.83M 0s\n",
      "   550K .......... .......... .......... .......... ...       100% 12.2M=0.1s\n",
      "\n",
      "2022-10-31 20:21:03 (5.21 MB/s) - 'nlp_getting_started.zip.4' saved [607343/607343]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data (same as from Kaggle)\n",
    "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
    "\n",
    "# Unzip data\n",
    "unzip_data(\"nlp_getting_started.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37069212-a601-4fad-b3c6-200ff09518cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn .csv files into pandas DataFrame's\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4de20eab-5987-4475-8727-4cf3b4d74a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41db42b8-cb17-485a-80d8-d98dcab61ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The test data doesn't have a target (that's what we'd try to predict)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00ca1756-1f4e-406c-b338-8a605ea5abb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class?\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ad19921-f72d-4a2e-adf8-dcb2cc175198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 7613\n",
      "Total test samples: 3263\n",
      "Total samples: 10876\n"
     ]
    }
   ],
   "source": [
    "# How many samples total?\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33d97aaa-4cd0-4a11-ae90-4d028c91af90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "HAPPY 24 TWISTER!!! Thank you for all the laughs sticking by my side no matter what and also forÛ_ https://t.co/ttq9IlHp8W\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Kenya News (Chelsea talisman Eden Hazard keen to match Cristiano Ronaldo and Lionel Messi.)  Mipasho http://t.co/LxvLqVbc8r\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "When you lowkey already know you're gonna drown in school this year :) http://t.co/aCMrm833zq\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Refugio oil spill may have been costlier bigger than projected http://t.co/lzob8qOH1B\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "WHY THE DEEP ROADS THO HAHAHAHA IM SO TRAUMATISED BY THE DEEP ROADS LOLOL\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ca785d-d9cb-409b-ac78-92285061b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
    "                                                                            random_state=42) # random state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc9ecb24-abc3-4174-9717-f5533fef5b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first 10 training sentences and their labels\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b49ca741-a54a-42af-b1f8-b5ce5fdf2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94b149dc-539b-4bec-8385-08d9d98f5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = TextVectorization(max_tokens=None,\n",
    "                                    standardize=\"lower_and_strip_punctuation\",\n",
    "                                    split=\"whitespace\",\n",
    "                                    ngrams=None,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ed86d0-b66d-40cd-a660-f172a413cf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find average number of tokens/words in training tweets\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e24040e-0a8c-4da4-a5b9-489b69d5532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode='int',\n",
    "                                    output_sequence_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bd30c6a-e0a5-4053-b654-0d30917edf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbb8b9fc-0981-4932-84cf-41c21b95d303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create sample sentence\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1e05fae-11ac-459b-b1cc-91ef583152cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "11 puncture wounds      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 968, 4834,  523,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choose random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed4630ca-8941-4935-8d1b-4a4164a510ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "#Get unique words in vocab\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5] #5 most common tokens\n",
    "bottom_5_words = words_in_vocab[-5:]# 5 least common tokens\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07cf86f0-79d9-408f-80be-fbd5fc5880d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x26929034280>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                             output_dim=128,\n",
    "                             embeddings_initializer='uniform',\n",
    "                             input_length=max_length,\n",
    "                             name='embedding_1'\n",
    "                            )\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cf6efa1-3e23-4444-8a94-c1476c728ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "#iphone #twist Ultimate #preparedness library: http://t.co/ksgmY0D0Mx Prepare Yourself For Any Catastrophe. Ov http://t.co/MZK0PFogI7      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.0010178 , -0.03880132, -0.01127563, ...,  0.00825617,\n",
       "         -0.04927752, -0.00497234],\n",
       "        [ 0.04587283,  0.01872038,  0.00884764, ..., -0.03619492,\n",
       "         -0.02061249, -0.03794064],\n",
       "        [ 0.02808792,  0.04362499,  0.03932795, ...,  0.04685874,\n",
       "         -0.02181524,  0.03074307],\n",
       "        ...,\n",
       "        [ 0.03977952, -0.03782602, -0.03646283, ...,  0.00236253,\n",
       "          0.03332629,  0.02803668],\n",
       "        [ 0.01645621, -0.00589932, -0.01471175, ..., -0.02511839,\n",
       "          0.00912381, -0.00024097],\n",
       "        [ 0.01645621, -0.00589932, -0.01471175, ..., -0.02511839,\n",
       "          0.00912381, -0.00024097]]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "#embed random sentence\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f60aabbd-abaa-4bb2-8a0f-a19f08fcdf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([ 1.01779774e-03, -3.88013236e-02, -1.12756267e-02, -1.47443786e-02,\n",
       "       -4.28895019e-02,  4.45820428e-02,  4.73705269e-02, -3.24172005e-02,\n",
       "        3.73673923e-02, -4.16718945e-02, -3.71053107e-02, -3.83436792e-02,\n",
       "       -4.86301780e-02,  1.65980719e-02, -2.02744957e-02, -1.90378316e-02,\n",
       "        9.81440395e-03,  1.90672539e-02, -2.13007331e-02,  1.58449449e-02,\n",
       "        2.27000751e-02,  3.99191119e-02, -1.29036903e-02,  1.07263550e-02,\n",
       "       -7.89596885e-03, -3.22480947e-02, -4.24733534e-02,  9.54698399e-03,\n",
       "        2.29340680e-02, -1.60842538e-02, -1.10437050e-02, -3.93310189e-02,\n",
       "       -1.38020627e-02,  2.86242999e-02, -3.01071890e-02,  2.18748488e-02,\n",
       "        2.90810727e-02, -3.30815911e-02, -9.76644456e-04,  1.91785432e-02,\n",
       "       -2.00281385e-02, -4.96787205e-02,  6.71118498e-03, -2.38791108e-05,\n",
       "        6.76850230e-03, -1.10270754e-02,  2.22495832e-02, -3.46133821e-02,\n",
       "       -3.67922783e-02, -1.49189830e-02, -2.82469280e-02, -6.92985952e-04,\n",
       "       -4.24017571e-02,  3.38771008e-02, -2.69105565e-02,  5.64832613e-03,\n",
       "        7.75212049e-03,  1.78530477e-02, -2.59840973e-02,  4.94168065e-02,\n",
       "        3.85443009e-02, -1.64137594e-02, -4.32423726e-02,  3.23186852e-02,\n",
       "       -3.92050371e-02,  2.07420476e-02,  1.65701993e-02, -2.25553643e-02,\n",
       "       -8.28659534e-03,  3.31082083e-02, -3.91948000e-02,  1.39915608e-02,\n",
       "       -3.37790996e-02, -1.11392140e-02, -3.29407007e-02,  3.35288756e-02,\n",
       "       -3.85190733e-02, -1.58288702e-02, -2.01509837e-02, -4.76366058e-02,\n",
       "       -3.60407457e-02,  1.02051981e-02,  2.28382833e-02, -8.82110745e-03,\n",
       "        4.21282537e-02,  2.46555321e-02, -4.37369347e-02, -1.86034292e-03,\n",
       "        4.53601740e-02,  1.52087323e-02,  3.59508432e-02,  1.66678168e-02,\n",
       "       -3.50067392e-02, -1.05119720e-02, -3.16741243e-02,  5.81145287e-03,\n",
       "        1.26063712e-02, -1.33453682e-03, -4.21190038e-02, -7.31359795e-03,\n",
       "        4.44277637e-02, -2.32102033e-02,  3.21624763e-02, -2.79599912e-02,\n",
       "        7.89005682e-03, -3.53187807e-02, -4.06276099e-02,  2.12258957e-02,\n",
       "       -4.83945273e-02, -3.61381061e-02,  3.68868001e-02,  4.36014868e-02,\n",
       "       -2.81201843e-02,  4.96932976e-02,  1.74337067e-02, -3.77277285e-03,\n",
       "        4.80571128e-02, -4.96393330e-02, -1.00437775e-02,  2.27303766e-02,\n",
       "        1.35106929e-02, -4.23890352e-03, -4.95855324e-02, -4.77856398e-03,\n",
       "       -2.90282853e-02,  8.25617462e-03, -4.92775217e-02, -4.97233868e-03],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embed[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35ff83bc-7e2d-4ef8-be41-c427ae0b7e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#create tokenization and modelling pipeline\n",
    "\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06d78448-c938-4d40-814d-27b241bc0cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8855eb11-dd1c-417c-9f22-ba0aaeb48560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8b7a292-8fd3-4442-a6eb-f1c6a021de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "  \n",
    "    Args:\n",
    "    -----\n",
    "    y_true = true labels in the form of a 1D array\n",
    "    y_pred = predicted labels in the form of a 1D array\n",
    "  \n",
    "    Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "    \"\"\"\n",
    "    # Calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    \n",
    "    # Calculate model precision, recall and f1 score using \"weighted\" average\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                    \"precision\": model_precision,\n",
    "                    \"recall\": model_recall,\n",
    "                    \"f1\": model_f1}\n",
    "    \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9582e90d-2763-435b-b1d6-cc23db23f486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels,\n",
    "                                     y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b32762ce-87aa-4ce4-a21c-02390be1d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 1\n",
    "\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f510e280-9e54-4242-b31c-ec1cde6e99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype='string') # inputs are 1D strings\n",
    "x = text_vectorizer(inputs) # text input into numbers\n",
    "x = embedding(x) # create embedding of numerized numbers\n",
    "x = layers.GlobalAveragePooling1D()(x) # \n",
    "\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_1 = tf.keras.Model(inputs, outputs, name='model_1_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb4a3cde-612b-4e28-b2b4-45b9c682abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af581f62-dce9-45a6-9d3a-8b6c0e802ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2dc9a4c-b9a7-47ae-8dd5-e07fa70bbc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/simple_dense_model/20221031-202106\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 3s 6ms/step - loss: 0.6094 - accuracy: 0.6916 - val_loss: 0.5357 - val_accuracy: 0.7572\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4410 - accuracy: 0.8189 - val_loss: 0.4691 - val_accuracy: 0.7848\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.3463 - accuracy: 0.8605 - val_loss: 0.4590 - val_accuracy: 0.7900\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2848 - accuracy: 0.8923 - val_loss: 0.4641 - val_accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2380 - accuracy: 0.9118 - val_loss: 0.4767 - val_accuracy: 0.7874\n"
     ]
    }
   ],
   "source": [
    "model_1_history = model_1.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                                    experiment_name=\"simple_dense_model\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f00e4b0b-b5d0-40fe-b374-6fbc48f586dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 0.4767 - accuracy: 0.7874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4766846001148224, 0.787401556968689]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the results\n",
    "model_1.evaluate(val_sentences, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e513f3d2-fc38-4854-90cd-081b0d0e03f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_1/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n",
       " array([[ 0.00073163,  0.01504798, -0.03425454, ..., -0.0440354 ,\n",
       "         -0.01042281,  0.01876437],\n",
       "        [ 0.04135862, -0.03945084, -0.0381194 , ...,  0.00464737,\n",
       "          0.03163552,  0.029283  ],\n",
       "        [ 0.00684032,  0.05363132, -0.00241555, ..., -0.07082176,\n",
       "         -0.04750706,  0.01448254],\n",
       "        ...,\n",
       "        [-0.03301444, -0.0052493 , -0.04209725, ...,  0.02028764,\n",
       "          0.00308807,  0.02215792],\n",
       "        [ 0.00692343,  0.05942352, -0.01975194, ..., -0.06199061,\n",
       "         -0.01018393,  0.03510419],\n",
       "        [-0.0372346 ,  0.06267187, -0.07451147, ..., -0.02367217,\n",
       "         -0.0864333 ,  0.01742155]], dtype=float32)>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d49cc94-72aa-40c1-8388-59f1dd370423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "print(embed_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6e0505a-1029-42d7-94d2-301e6ea6d65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4048821 ],\n",
       "       [0.7443312 ],\n",
       "       [0.997895  ],\n",
       "       [0.10889999],\n",
       "       [0.11143529],\n",
       "       [0.93556094],\n",
       "       [0.9134595 ],\n",
       "       [0.9925345 ],\n",
       "       [0.97156817],\n",
       "       [0.26570338]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions (these come back in the form of probabilities)\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs[:10] # only print out the first 10 prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ce85c3f-5c2c-44c3-9ed6-826d8420e50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn prediction probabilities into single-dimension tensor of floats\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\n",
    "model_1_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dff6f2f-c520-4f8b-b5fd-00b981731811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.74015748031496,\n",
       " 'precision': 0.7914920592553047,\n",
       " 'recall': 0.7874015748031497,\n",
       " 'f1': 0.7846966492209201}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_1 metrics\n",
    "model_1_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c47e7016-6405-44a7-8cdd-8edb45f3f5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is our simple Keras model better than our baseline model?\n",
    "import numpy as np\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "336accc2-83ab-44bb-927c-76a097ccd3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 78.74, Difference: -0.52\n",
      "Baseline precision: 0.81, New precision: 0.79, Difference: -0.02\n",
      "Baseline recall: 0.79, New recall: 0.79, Difference: -0.01\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.00\n"
     ]
    }
   ],
   "source": [
    "# Create a helper function to compare our baseline results to new model results\n",
    "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
    "    for key, value in baseline_results.items():\n",
    "        print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")\n",
    "\n",
    "compare_baseline_to_new_results(baseline_results=baseline_results, \n",
    "                                new_model_results=model_1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "986dd6a1-9aae-4407-a9e3-38639fa14325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "723f291c-b4c5-4ed2-a318-07aed355c5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "420e577e-3bda-4f4d-833e-d7e0b8cf1674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Get the weight matrix of embedding layer \n",
    "# (these are the numerical patterns between the text in the training dataset the model has learned)\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "print(embed_weights.shape) # same size as vocab size and embedding_dim (each word is a embedding_dim size vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b391129-7b32-4cab-a7e6-5954e879f2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15, 128)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer='uniform',\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_2\"\n",
    "                                    )\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_2_embedding(x)\n",
    "print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "print(x.shape)\n",
    "outputs=layers.Dense(1, activation = 'sigmoid')(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6c9b757-ca0c-4aa9-8aec-eb010014266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b088b4b2-97dd-40a8-addc-a77ec9173d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a367b5ce-fd56-44e5-9e9f-6a3a382af71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/LSTM/20221031-202115\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 9ms/step - loss: 0.5100 - accuracy: 0.7416 - val_loss: 0.4566 - val_accuracy: 0.7835\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.3175 - accuracy: 0.8717 - val_loss: 0.5138 - val_accuracy: 0.7756\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.2201 - accuracy: 0.9152 - val_loss: 0.5858 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.1556 - accuracy: 0.9428 - val_loss: 0.6041 - val_accuracy: 0.7743\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.1076 - accuracy: 0.9594 - val_loss: 0.8745 - val_accuracy: 0.7507\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_2_history = model_2.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
    "                                                                     \"LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8d81c42-5e79-47dd-8268-8892be869bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((762, 1),\n",
       " array([[0.00714374],\n",
       "        [0.7874937 ],\n",
       "        [0.99963737],\n",
       "        [0.05692937],\n",
       "        [0.00258346],\n",
       "        [0.9996232 ],\n",
       "        [0.9216303 ],\n",
       "        [0.99979883],\n",
       "        [0.999495  ],\n",
       "        [0.6644871 ]], dtype=float32))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9be5faf-28dc-440d-9084-655b4c838c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round out predictions and reduce to 1-dimensional array\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14f68349-af93-4d89-85a8-bd1d271df199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.06561679790026,\n",
       " 'precision': 0.7510077975908164,\n",
       " 'recall': 0.7506561679790026,\n",
       " 'f1': 0.7489268622514025}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Calculate LSTM model results\n",
    "model_2_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6bc31e7-e627-46cc-bfe1-2bb3c18f0161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 75.07, Difference: -4.20\n",
      "Baseline precision: 0.81, New precision: 0.75, Difference: -0.06\n",
      "Baseline recall: 0.79, New recall: 0.75, Difference: -0.04\n",
      "Baseline f1: 0.79, New f1: 0.75, Difference: -0.04\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results, model_2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77797f51-22b0-455e-ba14-2463ebd05729",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer='uniform',\n",
    "                                     input_length=max_length,\n",
    "                                     name='embedding_3'\n",
    "                                     )\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_3_embedding(x)\n",
    "x = layers.GRU(64)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cebacf93-5f7c-4878-9ee7-882529a06145",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(loss=\"binary_crossentropy\",\n",
    "                          optimizer=tf.keras.optimizers.Adam(),\n",
    "                          metrics=['accuracy']\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9d0a764-f3af-427b-8214-42b42e96c952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d4ffdfd-2cf3-4a79-917b-3031202faf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/GRU/20221031-202126\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 3s 8ms/step - loss: 0.5242 - accuracy: 0.7314 - val_loss: 0.4553 - val_accuracy: 0.7769\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3195 - accuracy: 0.8695 - val_loss: 0.4937 - val_accuracy: 0.7808\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.2197 - accuracy: 0.9181 - val_loss: 0.5607 - val_accuracy: 0.7743\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.1599 - accuracy: 0.9441 - val_loss: 0.6220 - val_accuracy: 0.7782\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.1221 - accuracy: 0.9584 - val_loss: 0.6205 - val_accuracy: 0.7677\n"
     ]
    }
   ],
   "source": [
    "model_3_history = model_3.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5, \n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"GRU\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d765499-a723-4925-a17d-6359625eb45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((762, 1),\n",
       " array([[0.3331606 ],\n",
       "        [0.87741786],\n",
       "        [0.9980248 ],\n",
       "        [0.11560916],\n",
       "        [0.01236071],\n",
       "        [0.9925644 ],\n",
       "        [0.62147355],\n",
       "        [0.99813336],\n",
       "        [0.9982374 ],\n",
       "        [0.50195044]], dtype=float32))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation data\n",
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs.shape, model_3_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49c9f2ba-fb70-4293-a241-8d82b17ae233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to prediction classes\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9bd8932a-70a9-4233-8921-f3a9b1b5d32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.77165354330708,\n",
       " 'precision': 0.7675450859410361,\n",
       " 'recall': 0.7677165354330708,\n",
       " 'f1': 0.7667932666650168}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcuate model_3 results\n",
    "model_3_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33c8b48d-814d-420f-9958-faf81d86df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 76.77, Difference: -2.49\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.04\n",
      "Baseline recall: 0.79, New recall: 0.77, Difference: -0.02\n",
      "Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"
     ]
    }
   ],
   "source": [
    "# Compare to baseline\n",
    "compare_baseline_to_new_results(baseline_results, model_3_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0e3d92f-7910-42d7-8afd-93f1aa918d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_4_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                    output_dim=128,\n",
    "                                    embeddings_initializer='uniform',\n",
    "                                    input_length=max_length,\n",
    "                                    name='embedding_4')\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_4_embedding(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name='model_4_Bidirectional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2707f48f-8f50-4c69-a841-6d90e414e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(loss='binary_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy']\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5ec5b93-1702-4808-b53a-d712db3d34b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3830c75d-83e6-45c8-ab46-8ee9c6326b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/bidirectional_RNN/20221031-202135\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 14ms/step - loss: 0.5093 - accuracy: 0.7481 - val_loss: 0.4606 - val_accuracy: 0.7795\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.3135 - accuracy: 0.8708 - val_loss: 0.5144 - val_accuracy: 0.7690\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.2150 - accuracy: 0.9178 - val_loss: 0.5626 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.1523 - accuracy: 0.9469 - val_loss: 0.6365 - val_accuracy: 0.7769\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.1083 - accuracy: 0.9639 - val_loss: 0.6510 - val_accuracy: 0.7664\n"
     ]
    }
   ],
   "source": [
    "model_4_history = model_4.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"bidirectional_RNN\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b04c8330-7cdf-457e-9c6f-a0df9df46181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03997265],\n",
       "       [0.82793885],\n",
       "       [0.9984213 ],\n",
       "       [0.13520303],\n",
       "       [0.00311086],\n",
       "       [0.99219877],\n",
       "       [0.9553587 ],\n",
       "       [0.99945587],\n",
       "       [0.998982  ],\n",
       "       [0.28128412]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07ce30a8-1baa-48b0-854d-3c7e18bb15a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3cb1a26b-47c5-4d40-b7b2-b7aac88ecdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.64041994750657,\n",
       " 'precision': 0.7665895370389821,\n",
       " 'recall': 0.7664041994750657,\n",
       " 'f1': 0.7651213533864446}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_results = calculate_results(val_labels, model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8cc5ac9c-622e-470f-ab55-67148dabc736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 76.64, Difference: -2.62\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.04\n",
      "Baseline recall: 0.79, New recall: 0.77, Difference: -0.03\n",
      "Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results, model_4_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84e915b4-7297-407c-a903-2b7baea3eef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"]))\n",
    "conv_1d = layers.Conv1D(filters=32, kernel_size=5, activation='relu')\n",
    "conv_1d_output = conv_1d(embedding_test)\n",
    "max_pool = layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output)\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f70e59fe-5acd-4efa-b3cc-e3fbcd2deb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       " array([[[ 0.02534913, -0.03109058,  0.00285616, ..., -0.00783161,\n",
       "          -0.02685578, -0.04434132],\n",
       "         [-0.0658626 ,  0.09451492, -0.01477603, ..., -0.00657782,\n",
       "          -0.04238789,  0.07777893],\n",
       "         [-0.04803652, -0.00709756, -0.02330892, ..., -0.01807332,\n",
       "           0.02351034,  0.02676385],\n",
       "         ...,\n",
       "         [ 0.00073163,  0.01504798, -0.03425454, ..., -0.0440354 ,\n",
       "          -0.01042281,  0.01876437],\n",
       "         [ 0.00073163,  0.01504798, -0.03425454, ..., -0.0440354 ,\n",
       "          -0.01042281,  0.01876437],\n",
       "         [ 0.00073163,  0.01504798, -0.03425454, ..., -0.0440354 ,\n",
       "          -0.01042281,  0.01876437]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n",
       " array([[[0.08324983, 0.00648714, 0.        , 0.03983572, 0.        ,\n",
       "          0.01144416, 0.0041625 , 0.02288389, 0.        , 0.00900978,\n",
       "          0.        , 0.        , 0.03401771, 0.06408276, 0.08103722,\n",
       "          0.00409015, 0.01579617, 0.        , 0.07930177, 0.        ,\n",
       "          0.        , 0.        , 0.14525086, 0.        , 0.        ,\n",
       "          0.        , 0.03682078, 0.06534284, 0.        , 0.        ,\n",
       "          0.05094624, 0.        ],\n",
       "         [0.        , 0.05387186, 0.        , 0.11491335, 0.        ,\n",
       "          0.        , 0.16237082, 0.        , 0.        , 0.00171252,\n",
       "          0.14336711, 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.01197934, 0.        , 0.        , 0.13551371, 0.0040106 ,\n",
       "          0.10309825, 0.09445542, 0.08390296, 0.        , 0.04213034,\n",
       "          0.04487597, 0.0656046 , 0.        , 0.02272683, 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.03683226, 0.04895764, 0.        , 0.15324754, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.04650314, 0.00496457, 0.07349403, 0.01608642,\n",
       "          0.        , 0.02779122, 0.        , 0.08080561, 0.01403175,\n",
       "          0.        , 0.03768813, 0.10382783, 0.        , 0.03361665,\n",
       "          0.        , 0.02577607, 0.00140356, 0.        , 0.        ,\n",
       "          0.03211498, 0.        ],\n",
       "         [0.00887821, 0.10450975, 0.        , 0.06974537, 0.02328688,\n",
       "          0.        , 0.04052209, 0.        , 0.        , 0.02733763,\n",
       "          0.08674344, 0.        , 0.        , 0.06129853, 0.02007268,\n",
       "          0.        , 0.        , 0.        , 0.03364265, 0.        ,\n",
       "          0.04525332, 0.05219701, 0.06375708, 0.        , 0.        ,\n",
       "          0.00774409, 0.0027347 , 0.        , 0.        , 0.00499634,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.02369068, 0.        , 0.0582762 , 0.05297641,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.01719719, 0.0293682 , 0.00466103, 0.06879887, 0.01944806,\n",
       "          0.01585531, 0.01294544, 0.        , 0.0686653 , 0.        ,\n",
       "          0.00623766, 0.03514048, 0.02407534, 0.        , 0.05979818,\n",
       "          0.        , 0.01170144, 0.        , 0.        , 0.        ,\n",
       "          0.04444929, 0.        ],\n",
       "         [0.03544863, 0.        , 0.        , 0.05054975, 0.06105437,\n",
       "          0.        , 0.0099743 , 0.01403009, 0.        , 0.01680727,\n",
       "          0.03148508, 0.03889386, 0.        , 0.07710679, 0.00590969,\n",
       "          0.        , 0.00263032, 0.        , 0.08935824, 0.        ,\n",
       "          0.        , 0.05331147, 0.05227952, 0.        , 0.06658387,\n",
       "          0.01881708, 0.02448699, 0.        , 0.        , 0.        ,\n",
       "          0.02008457, 0.        ],\n",
       "         [0.03544863, 0.        , 0.        , 0.05054974, 0.06105437,\n",
       "          0.        , 0.0099743 , 0.0140301 , 0.        , 0.01680727,\n",
       "          0.03148507, 0.03889387, 0.        , 0.0771068 , 0.0059097 ,\n",
       "          0.        , 0.00263033, 0.        , 0.08935823, 0.        ,\n",
       "          0.        , 0.05331147, 0.05227952, 0.        , 0.06658387,\n",
       "          0.01881707, 0.02448698, 0.        , 0.        , 0.        ,\n",
       "          0.02008458, 0.        ],\n",
       "         [0.03544864, 0.        , 0.        , 0.05054974, 0.06105438,\n",
       "          0.        , 0.00997429, 0.0140301 , 0.        , 0.01680727,\n",
       "          0.03148508, 0.03889387, 0.        , 0.07710679, 0.0059097 ,\n",
       "          0.        , 0.00263034, 0.        , 0.08935824, 0.        ,\n",
       "          0.        , 0.05331148, 0.05227952, 0.        , 0.06658386,\n",
       "          0.01881707, 0.02448698, 0.        , 0.        , 0.        ,\n",
       "          0.02008458, 0.        ],\n",
       "         [0.03544864, 0.        , 0.        , 0.05054973, 0.06105438,\n",
       "          0.        , 0.0099743 , 0.0140301 , 0.        , 0.01680727,\n",
       "          0.03148507, 0.03889386, 0.        , 0.0771068 , 0.0059097 ,\n",
       "          0.        , 0.00263033, 0.        , 0.08935824, 0.        ,\n",
       "          0.        , 0.05331147, 0.05227954, 0.        , 0.06658389,\n",
       "          0.01881708, 0.02448698, 0.        , 0.        , 0.        ,\n",
       "          0.02008457, 0.        ],\n",
       "         [0.03544864, 0.        , 0.        , 0.05054975, 0.06105437,\n",
       "          0.        , 0.0099743 , 0.0140301 , 0.        , 0.01680727,\n",
       "          0.03148508, 0.03889387, 0.        , 0.07710679, 0.00590969,\n",
       "          0.        , 0.00263033, 0.        , 0.08935824, 0.        ,\n",
       "          0.        , 0.05331147, 0.05227952, 0.        , 0.06658388,\n",
       "          0.01881708, 0.02448698, 0.        , 0.        , 0.        ,\n",
       "          0.02008456, 0.        ],\n",
       "         [0.03544864, 0.        , 0.        , 0.05054974, 0.06105438,\n",
       "          0.        , 0.00997429, 0.0140301 , 0.        , 0.01680727,\n",
       "          0.03148508, 0.03889386, 0.        , 0.07710679, 0.0059097 ,\n",
       "          0.        , 0.00263032, 0.        , 0.08935824, 0.        ,\n",
       "          0.        , 0.05331148, 0.05227953, 0.        , 0.06658387,\n",
       "          0.01881707, 0.02448698, 0.        , 0.        , 0.        ,\n",
       "          0.02008457, 0.        ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       " array([[0.08324983, 0.10450975, 0.        , 0.15324754, 0.06105438,\n",
       "         0.01144416, 0.16237082, 0.02288389, 0.        , 0.02733763,\n",
       "         0.14336711, 0.04650314, 0.03401771, 0.0771068 , 0.08103722,\n",
       "         0.01585531, 0.02779122, 0.        , 0.13551371, 0.01403175,\n",
       "         0.10309825, 0.09445542, 0.14525086, 0.        , 0.06658389,\n",
       "         0.04487597, 0.0656046 , 0.06534284, 0.02272683, 0.00499634,\n",
       "         0.05094624, 0.        ]], dtype=float32)>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the outputs of each layer\n",
    "embedding_test[:1], conv_1d_output[:1], max_pool_output[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88d14a18-88b6-467e-a178-11489a0407c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 11, 32)            20512     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,300,545\n",
      "Trainable params: 1,300,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_5_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer='uniform',\n",
    "                                     input_length=max_length,\n",
    "                                     name='embedding_5')\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_5_embedding(x)\n",
    "x = layers.Conv1D(filters=32, kernel_size=5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name='model_5_Conv1D')\n",
    "\n",
    "model_5.compile(loss='binary_crossentropy',\n",
    "                optimizer='Adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_5.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e09e1d7-ea38-466f-81d3-7293e9e22e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/Conv1D/20221031-202151\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.5654 - accuracy: 0.7152 - val_loss: 0.4729 - val_accuracy: 0.7769\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.3382 - accuracy: 0.8626 - val_loss: 0.4748 - val_accuracy: 0.7795\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.2071 - accuracy: 0.9231 - val_loss: 0.5423 - val_accuracy: 0.7664\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.1307 - accuracy: 0.9574 - val_loss: 0.6155 - val_accuracy: 0.7743\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0934 - accuracy: 0.9695 - val_loss: 0.6714 - val_accuracy: 0.7848\n"
     ]
    }
   ],
   "source": [
    "model_5_history = model_5.fit(train_sentences,\n",
    "                             train_labels,\n",
    "                             epochs=5,\n",
    "                             validation_data=(val_sentences, val_labels),\n",
    "                             callbacks=[create_tensorboard_callback(SAVE_DIR, 'Conv1D')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a063330-8ad2-4780-8c14-0570f17d7c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23463732],\n",
       "       [0.74668884],\n",
       "       [0.9992914 ],\n",
       "       [0.04693327],\n",
       "       [0.01458224]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b208f0a1-eb46-437c-94c6-fae1b4e3e8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 1., 1., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b4cd4b8-d70d-48a8-8775-991825ba618d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.4776902887139,\n",
       " 'precision': 0.789165199286798,\n",
       " 'recall': 0.7847769028871391,\n",
       " 'f1': 0.7818959205825942}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_5_preds)\n",
    "\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "083ed313-5b4a-4600-9a3d-21aef9c724bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 78.48, Difference: -0.79\n",
      "Baseline precision: 0.81, New precision: 0.79, Difference: -0.02\n",
      "Baseline recall: 0.79, New recall: 0.78, Difference: -0.01\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.00\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results, model_5_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dc262373-2719-4986-b4bc-d775ec5c062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01157027  0.0248591   0.02878048 -0.012715    0.03971538  0.0882776\n",
      "  0.02680985  0.05589838 -0.01068729 -0.00597292  0.00639323 -0.0181952\n",
      "  0.00030814  0.09105888  0.05874645 -0.03180628  0.01512474 -0.05162929\n",
      "  0.00991367 -0.06865346 -0.04209305  0.0267898   0.03011008  0.00321069\n",
      " -0.00337971 -0.04787356  0.02266719 -0.00985925 -0.04063613 -0.01292093\n",
      " -0.04666384  0.056303   -0.03949255  0.00517688  0.02495828 -0.07014441\n",
      "  0.02871508  0.04947684 -0.00633978 -0.08960193  0.02807117 -0.00808362\n",
      " -0.01360601  0.0599865  -0.10361787 -0.05195374  0.00232955 -0.0233253\n",
      " -0.03758105  0.03327729], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder\n",
    "embed_samples = embed([sample_sentence,\n",
    "                      \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
    "\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7b3aaad7-67e1-4078-ba40-6feacf773cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each sentence has been encoded into a 512 dimension vector\n",
    "embed_samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "03471131-52a6-4779-831e-b6bd2fc09b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[],\n",
    "                                        dtype=tf.string,\n",
    "                                        trainable=False,\n",
    "                                        name='USE'\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1f297fed-f677-45de-9665-4f6aeb1dd320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "], name='model_6_USE')\n",
    "\n",
    "model_6.compile(loss='binary_crossentropy',\n",
    "                optimizer='Adam',\n",
    "                metrics=['accuracy']\n",
    "               )\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2d2c799-034f-4fd9-90ce-5592e9fa6612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/tf_hub_sentence_encoder/20221031-202212\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 18ms/step - loss: 0.5008 - accuracy: 0.7892 - val_loss: 0.4479 - val_accuracy: 0.7966\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.4144 - accuracy: 0.8130 - val_loss: 0.4369 - val_accuracy: 0.8058\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.3998 - accuracy: 0.8213 - val_loss: 0.4329 - val_accuracy: 0.8110\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.3925 - accuracy: 0.8260 - val_loss: 0.4289 - val_accuracy: 0.8110\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.3861 - accuracy: 0.8285 - val_loss: 0.4309 - val_accuracy: 0.8097\n"
     ]
    }
   ],
   "source": [
    "model_6_history = model_6.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
    "                                                                     \"tf_hub_sentence_encoder\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bacc23-1316-48a8-95cd-91c4d42b444b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e2c036-0fda-4988-bf8d-02c337f0905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560091a2-986c-4b56-a91e-3b7a95901c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
